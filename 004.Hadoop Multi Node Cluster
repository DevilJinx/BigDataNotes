Hadoop Multi Node Cluster

data1：
cd /opt/software/hadoop-2.8.1/etc/hadoop
vi core-site.xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master:9000</value>
    </property>
</configuration>

vi yarn-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.shuffleHandler</value>
    </property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>master:8025</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>master:8030</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>master:8050</value>
</property>
</configuration>

vi mapred-site.xml
<configuration>
    <property>
        <name>mapred.job.tracker</name>
        <value>master:54311</value>
    </property>
</configuration>

vi hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/hadoop_data/hdfs/datanode</value>
    </property>
</configuration>

vi /etc/hosts
10.37.129.38  data1     f708372f-063f-374e-8a6c-6f4767e4bf9d    00:1c:42:47:7f:10
10.37.129.39  data2     23652007-7ee5-3610-b4a2-cd168dcdb19b    00:1c:42:24:7d:ca
10.37.129.40  data3     691ab12f-fa5c-38a7-9197-881a9d578d5f    00:1c:42:2a:ab:7b
10.37.129.41  master    dfe84ea8-4f0d-377f-8f72-f873c6cd9fa9    00:1c:42:83:64:12

将data1复制到data2、data3、master，设置静态IP

master:
cd /usr/local/hadoop/etc/hadoop/

vi core-site.xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master:9000</value>
    </property>
</configuration>

vi hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/hadoop_data/hdfs/namenode</value>
    </property>
</configuration>

vi masters
master

vi slaves
data1
data2
data3

reboot

vi /etc/ssh/sshd_config
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile     .ssh/authorized_keys
service sshd restart

ssh-keygen -t dsa
cd .ssh/
cat id_dsa.pub >>  authorized_keys
chmod 600 authorized_keys
ssh themachine

在themachine上：
cat ~/.ssh/id_dsa.pub | ssh root@data1 'cat - >> ~/.ssh/authorized_keys'
cat ~/.ssh/id_dsa.pub | ssh root@data2 'cat - >> ~/.ssh/authorized_keys'
cat ~/.ssh/id_dsa.pub | ssh root@data3 'cat - >> ~/.ssh/authorized_keys'


master、data1、data2、data3关闭防火墙：
systemctl stop firewalld.service
systemctl disable firewalld.service
firewall-cmd --state
reboot

master连接到data1、data2、data3创建HDFS目录：
ssh data1
rm -rf /usr/local/hadoop/hadoop_data/hdfs/
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
chown -R root:root /usr/local/hadoop
exit

ssh data2
rm -rf /usr/local/hadoop/hadoop_data/hdfs/
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
chown -R root:root /usr/local/hadoop
exit

ssh data3
rm -rf /usr/local/hadoop/hadoop_data/hdfs/
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
chown -R root:root /usr/local/hadoop
exit

rm -rf /usr/local/hadoop/hadoop_data/hdfs
mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode
chown -R root:root /usr/local/hadoop/

hdfs namenode -format
start-all.sh
jps
ssh data1
jps
exit

打开Hadoop ResourceManager Web界面：
http://master:8088/
打开Namenode Web界面：
http://master:50070/
停止Hadoop Multi Node Cluster:
stop-all.sh

